# 异构计算架构：让 AI 加速器集群高效协作

> 如何将 8 张 AI 加速卡组织成一个高效的"神经游戏引擎"？本文深入解读异构流水线设计、最优资源分配和投机执行等核心架构创新。

## 问题的起点：为什么不能简单地"堆卡"？

直觉上，要让生成式游戏引擎跑得更快，最简单的办法就是——多加几张卡。但现实远比这复杂。

在传统的分布式推理中，常见的策略有：

- **流水线并行（Pipeline Parallelism）**：将模型的不同层分配到不同设备
- **张量并行（Tensor Parallelism）**：将同一层的计算拆分到多个设备
- **数据并行（Data Parallelism）**：每个设备处理不同的输入数据

但这些策略都假设工作负载是**同质的**——所有组件的计算特征相似。而生成式游戏引擎恰恰违背了这个假设。

## 关键洞察：计算瓶颈与内存瓶颈的根本异质性

论文的核心发现是：

> **世界模型（DiT）和图像解码器（VAE）具有根本不同的计算特征，必须用不同的并行策略来处理。**

| 组件 | 核心运算 | 瓶颈类型 | 最佳并行策略 |
|------|---------|---------|-------------|
| **世界模型（DiT）** | Attention 矩阵乘法 | **计算瓶颈** | 序列并行（Ulysses） |
| **解码器（VAE）** | 特征图读写 | **内存瓶颈** | 空间并行 |

### 序列并行——让 DiT 跑得更快

DiT 使用 Self-Attention 机制，其计算复杂度与序列长度的平方成正比。论文采用 **Ulysses 序列并行**策略：将注意力头（Attention Heads）分配到不同的设备上。

具体来说，如果有 30 个注意力头和 5 张卡，每张卡处理 6 个头。设备之间需要通过 **All-to-All** 通信来交换 Query/Key/Value 张量。

DiT 的延迟可以建模为：

$$T_{\text{DiT}}(N_d) = \underbrace{\frac{\alpha}{N_d}}_{\text{计算时间（随卡数线性下降）}} + \underbrace{\beta \cdot \frac{N_d - 1}{N_d}}_{\text{通信开销（随卡数逐渐趋于常数）}}$$

这个公式揭示了一个重要的**权衡**：

- 增加 DiT 卡数 → 计算时间减少（好事）
- 但通信开销的比例趋近于 $\beta$（坏事）

### 空间并行——让 VAE 跑得更快

VAE 解码器的瓶颈在于内存带宽。论文采用**空间并行**策略：将特征图在宽度维度上切分，每张卡处理一个空间切片。

VAE 的延迟建模更简单：

$$T_{\text{VAE}}(N_v) \approx \frac{\mathcal{M}_{\text{VAE}}}{N_v \cdot BW_{\text{HBM}} \cdot \eta_{\text{eff}}}$$

由于是内存瓶颈，性能随**聚合内存带宽**线性提升，通信开销相对较低。

## 资源分配：一个精巧的离散优化问题

现在问题来了：**8 张卡，给 DiT 几张，给 VAE 几张？**

系统吞吐量受制于较慢的那个阶段（木桶效应）：

$$FPS(N_d, N_v) = \min\left(\frac{1}{T_{\text{DiT}}(N_d)},\ \frac{1}{T_{\text{VAE}}(N_v)}\right)$$

约束条件是 Ulysses 要求注意力头数 $H$ 必须能被 $N_d$ 整除。对于论文中使用的模型（$H=30$，总共 8 张卡），可行的分配方案为：

| 配置 | DiT 卡数 | VAE 卡数 | DiT 耗时 | FPS | 瓶颈 |
|------|---------|---------|---------|-----|------|
| 2 + 6 | 2 | 6 | 63.8 ms | 15.6 | DiT（算力不足） |
| 3 + 5 | 3 | 5 | 60.1 ms | 16.6 | DiT（通信过大） |
| **5 + 3** | **5** | **3** | **51.5 ms** | **19.4** | **平衡** |
| 6 + 2 | 6 | 2 | 31.6 ms | 18.3 | VAE（带宽不足） |

**5:3 分配是全局最优解！**

### 为什么 5:3 最优？

- **2:6**：DiT 只有 2 张卡，算力严重不足，成为瓶颈
- **3:5**：DiT 有 3 张卡，但 All-to-All 通信在 3 卡之间仍较重
- **5:3**：DiT 获得充足算力，通信开销可控；VAE 3 张卡的聚合带宽刚好满足需求
- **6:2**：DiT 算得很快（31.6ms），但 VAE 只有 2 张卡，内存带宽成为新的瓶颈

这个结果有一个深刻的含义：**最优解不是让两个阶段都"最快"，而是让它们"一样快"**——即消除瓶颈不对称性。

## 流水线编排：Round-Robin 异步调度

解决了"给多少卡"的问题，接下来是"怎么调度"的问题。

论文采用了 **Round-Robin（轮转调度）** 策略。虽然单帧的物理计算时间约为 147ms（VAE 解码主导），但通过流水线化，系统可以**将像素输出速率与 DiT 生成间隔对齐**。

工作流程如下：

```
时间轴 →
DiT:      [帧1] → [帧2] → [帧3] → [帧4] → ...
           │        │        │        │
VAE-1:    [解码帧1]       [解码帧4]       ...
VAE-2:         [解码帧2]       [解码帧5]  ...
VAE-3:              [解码帧3]       [帧6] ...
```

DiT 每生成一个潜空间表示（约 38ms），就通过 Round-Robin 将其分发给空闲的 VAE Worker。虽然每个 VAE Worker 解码一帧需要约 109ms，但 3 个 Worker 轮流工作，使得系统的**有效帧间隔**仅为 38ms（约 26.4 FPS）。

这就像餐厅里有 3 个厨师：虽然每道菜要烹饪 15 分钟，但只要间隔 5 分钟下一道单，每 5 分钟就能上一道菜。

## 投机动作预取：让延迟"消失"

即使优化到 38ms 的帧间隔，在高节奏游戏中仍然可能感知到延迟。论文引入了一个巧妙的机制——**投机动作预取（Speculative Action Prefetching）**。

### 原理

1. 一个轻量级的 LSTM 模型（2 层，128 隐藏单元）根据玩家最近的操作历史预测下一步操作
2. 加速器**提前**使用预测的操作来生成下一帧
3. 当实际输入到达时：
   - **命中（93% 的情况）**：直接显示预生成的帧，延迟几乎为零
   - **未命中（7% 的情况）**：丢弃预生成帧，重新生成

### 数学建模

有效延迟的数学表达式为：

$$\text{Latency}_{\text{eff}} = P_{\text{hit}} \cdot T_{\text{overhead}} + (1 - P_{\text{hit}}) \cdot (T_{\text{sys}} + T_{\text{overhead}})$$

代入实测数据：
- $P_{\text{hit}} \approx 93\%$（命中率）
- $T_{\text{sys}} \approx 38\text{ms}$（系统延迟）
- $T_{\text{overhead}} \approx 0.1\text{ms}$（系统开销）

$$\text{Latency}_{\text{eff}} \approx 0.93 \times 0.1 + 0.07 \times 38.1 \approx 2.7\text{ms}$$

**2.7 毫秒的感知延迟——比大多数人的反应时间（约 200ms）快近 100 倍！**

### 为什么 93% 的命中率是可信的？

在游戏中，大多数时间玩家都在做**连续性动作**——持续按住方向键、保持油门等。LSTM 只需要检测"玩家是否改变了操作"，这是一个相对简单的预测任务。只有在**突然变向、急刹车**等场景下才会预测失败。

即使预测失败，26.4 FPS 的帧率确保了修正帧能在一个垂直同步周期内送达。利用人类视觉系统的**变化盲视（Change Blindness）** 效应，短暂的画面不连续几乎不会被察觉。

## 控制平面与数据平面的彻底分离

论文架构的另一个重要设计原则是**控制平面（Control Plane）与数据平面（Data Plane）的彻底分离**：

```
┌──────────────────────────────────┐
│         控制平面（CPU Host）        │
│  • 用户输入捕获                    │
│  • 任务调度（Ray 框架）             │
│  • 投机动作预测（LSTM）             │
│  • 逻辑验证                       │
└──────────┬───────────────────────┘
           │ PCIe（仅传输控制信号）
┌──────────▼───────────────────────┐
│     数据平面（加速器集群）           │
│                                  │
│  [DiT-1]─[DiT-2]─[DiT-3]─...    │
│      │     HCCS 30GB/s           │
│  [VAE-1]─[VAE-2]─[VAE-3]        │
│                                  │
│  → 张量数据仅在加速器间流动        │
│  → 零拷贝数据流，避免PCIe瓶颈      │
└──────────────────────────────────┘
```

传统架构中，CPU 是"中央指挥官"——所有数据都要经过 CPU 才能到达 GPU。但在这个设计中：

- CPU 只负责**轻量级控制**（输入捕获、调度决策）
- 所有**重型张量运算**都在加速器集群内完成
- DiT 和 VAE 之间通过**高速互联（HCCS，30GB/s）** 直接通信
- **PCIe 总线不再是瓶颈**

这种设计确保了 720×480 分辨率所需的海量张量吞吐不会被 PCIe 总线限制。

## 小结：架构层面的关键创新

| 创新点 | 解决的问题 | 核心思想 |
|--------|-----------|---------|
| 异构流水线 | DiT 和 VAE 计算特征不同 | 不同组件用不同的并行策略 |
| 最优资源分配（5:3） | 如何分配有限的硬件资源 | 基于理论建模求解离散优化 |
| Round-Robin 调度 | 单帧延迟太高 | 流水线化隐藏 VAE 延迟 |
| 投机动作预取 | 输入-输出延迟 | 用 LSTM 预测下一步操作 |
| 控制-数据分离 | PCIe 带宽瓶颈 | 张量仅在加速器集群内流动 |

这些创新并非孤立存在——它们共同构成了一个**系统级的解决方案**。正如论文所强调的，突破内存墙不是某一层的优化，而是需要在集群级、芯片级和算法级**三个层次协同发力**。

---

*上一篇：[突破"内存墙"：高分辨率神经游戏的瓶颈与突围](./02-突破内存墙：高分辨率神经游戏的瓶颈与突围.md)*

*下一篇：[算子融合与流形外推：从芯片级到算法级的极致优化](./04-算子融合与流形外推：从芯片级到算法级的极致优化.md)*
